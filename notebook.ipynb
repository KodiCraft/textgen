{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple textgen setup\n",
    "\n",
    "The following notebook allows you to generate text using a pretrained model and tokenizer in a friendly interface.\n",
    "\n",
    "## Setup cells\n",
    "Only run these cells once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 15:48:59.666251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you may set the model and tokenizer you want to use. You can find a list of available models [here](https://huggingface.co/models?pipeline_tag=text-generation). By default, the notebook uses the `gpt-neo-2.7B` model with its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation setup\n",
    "\n",
    "The following cells define the helper functions that will be used for text generation. You shouldn't need to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful constants\n",
    "TEMPERATURE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFromString(prompt, num_tokens, temperature=TEMPERATURE):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    max_length = input_ids.shape[1] + num_tokens\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id)\n",
    "    return (tokenizer.decode(output[0], skip_special_tokens=True), len(output[0]) - len(input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(genFromString(\"Hey everyone! Welcome to the stream. Today, we'll be playing \", 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative generation setup\n",
    "\n",
    "These cells allow the iterative generator to be used, which allows you to generate text iteratively, only generating a certain amount of tokens at a time. This is useful for generating long texts, as it allows you to generate the text in chunks, and then combine the chunks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is merely a helper function for rendering the time\n",
    "def formatTime(seconds):\n",
    "    # Formats the time in a human-readable way\n",
    "    # XXms\n",
    "    # or XXsXXms\n",
    "    # or XXmXXsXXms\n",
    "    \n",
    "    if seconds < 1:\n",
    "        return '{:.2f}ms'.format(seconds * 1000)\n",
    "    elif seconds < 60:\n",
    "        return '{:.2f}s'.format(seconds)\n",
    "    else:\n",
    "        return '{:.2f}m{:.2f}s'.format(seconds // 60, seconds % 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The following cell contains the functions that are used for creating the generator. You can think of this as the \"frontend\" of the notebook, as it contains the functions that are used to interact with the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativeGen(initial_prompt, total_tokens=100, chunk_size=50, temperature=TEMPERATURE):\n",
    "    prompt = initial_prompt\n",
    "    tokens_generated = 0\n",
    "    while tokens_generated < total_tokens:\n",
    "        # Generate the next tokens\n",
    "        next_tokens, num_tokens = genFromString(prompt, chunk_size, temperature=temperature)\n",
    "\n",
    "        # The text generated is the prompt for the next iteration\n",
    "        prompt = next_tokens\n",
    "\n",
    "        # Update the number of tokens generated\n",
    "        tokens_generated += num_tokens\n",
    "        yield (prompt, tokens_generated)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generator():\n",
    "    # Create a widget that will display the generated text\n",
    "    text = widgets.Textarea()\n",
    "    # Set the text area to be wider than the default\n",
    "    text.layout.width = '75%'\n",
    "    \n",
    "    # Create a button to generate \"more text\"\n",
    "    moreButton = widgets.Button(description='More')\n",
    "    progressBar = widgets.IntProgress(\n",
    "        min = 0,\n",
    "        max = 100,\n",
    "        bar_style='success',\n",
    "        value = 0,\n",
    "    )\n",
    "    \n",
    "    # Create a slider to control the number of tokens per chunk\n",
    "    chunkSlider = widgets.IntSlider(\n",
    "        value = 50,\n",
    "        min = 1,\n",
    "        max = 100,\n",
    "        description = 'Tokens/chk:',\n",
    "        orientation = 'horizontal',\n",
    "    )\n",
    "    \n",
    "    # Create a slider to control the temperature\n",
    "    tempSlider = widgets.FloatSlider(\n",
    "        value = TEMPERATURE,\n",
    "        min = 0.1,\n",
    "        max = 1.0,\n",
    "        step = 0.1,\n",
    "        description = 'Temp:',\n",
    "        orientation = 'horizontal',\n",
    "    )\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        print('Generating more text...')\n",
    "        start = time.time()\n",
    "        text.disabled = True\n",
    "        moreButton.disabled = True\n",
    "        chunkSlider.disabled = True\n",
    "        tempSlider.disabled = True\n",
    "        progressBar.bar_style = 'info'\n",
    "        progressBar.value = 0\n",
    "        times = []\n",
    "        for prompt, tokens_generated in iterativeGen(text.value, 100, chunkSlider.value, temperature=tempSlider.value):\n",
    "            text.value = prompt\n",
    "            progressBar.value = tokens_generated\n",
    "            times += [time.time() - start]\n",
    "            start = time.time()\n",
    "        text.disabled = False\n",
    "        moreButton.disabled = False\n",
    "        chunkSlider.disabled = False\n",
    "        tempSlider.disabled = False\n",
    "        progressBar.bar_style = 'success'\n",
    "        \n",
    "        print('Done in {:.2f} seconds (avg {:.2f}s/chunk, {:.2f}s/token)! Generated {} total tokens over {} chunks.'.format(sum(times), sum(times) / len(times), sum(times) / tokens_generated, tokens_generated, len(times)))\n",
    "    \n",
    "    moreButton.on_click(on_button_clicked)\n",
    "    # Create a vertical box to hold the buttons\n",
    "    vbox = widgets.VBox([moreButton, chunkSlider, tempSlider, progressBar])\n",
    "    \n",
    "    # Create a horizontal box to hold the text and buttons\n",
    "    hbox = widgets.HBox([text, vbox])\n",
    "    display(hbox)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "This final cell creates the generator interface, which allows you to generate text using the model and tokenizer you selected above.\n",
    "\n",
    "To use it, begin by writing some text in the textbox below. Then, click on the \"More\" button to launch the generation.\n",
    "\n",
    "You may also use the two sliders to control the amount of text generated per chunk and the temperature of the generation. Lower tokens per chunk will display the text more quickly, but might decrease the quality of the text. Higher temperatures will increase the diversity of the text, but might decrease the quality of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e6d7429ca43c9889edb46946e83ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='', layout=Layout(width='75%')), VBox(children=(Button(description='More', styleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating more text...\n",
      "Done in 42.33 seconds (avg 21.16s/chunk, 0.42s/token)! Generated 100 total tokens over 2 chunks.\n"
     ]
    }
   ],
   "source": [
    "await generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
